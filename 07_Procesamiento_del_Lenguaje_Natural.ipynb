{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_Procesamiento_del_Lenguaje_Natural.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMQIEQFsksQISE6+lqCjFdZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IEXE-Tec/aprendizaje-maquina-2/blob/master/07_Procesamiento_del_Lenguaje_Natural.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kECscepGtsRT"
      },
      "source": [
        "Installing pycaret library:\n",
        "+ https://towardsdatascience.com/announcing-pycaret-2-0-39c11014540e \n",
        "+ https://pycaret.org/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_4YZ-A-ty8L"
      },
      "source": [
        "!pip install pycaret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUw9jGQ6e8zi",
        "outputId": "46dd277f-e4b4-47e2-c85a-3b7a0a024337",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "from pycaret.utils import version\n",
        "version()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA99ILJFHqa3"
      },
      "source": [
        "Installing Autocorrect:\n",
        "+ https://github.com/phatpiglet/autocorrect\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX_UZpHaHtYN",
        "outputId": "6d705128-eaaf-4908-80f8-4cc8e200f4bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "source": [
        "!pip install autocorrect"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autocorrect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/71/eb8c1f83439dfe6cbe1edb03be1f1110b242503b61950e7c292dd557c23e/autocorrect-2.2.2.tar.gz (621kB)\n",
            "\u001b[K     |████████████████████████████████| 624kB 2.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.2.2-cp36-none-any.whl size=621491 sha256=8a4ef5b6c5ebffc80c11cb6ee5312ddddf9e42082e437cf37ddb43676dfae36e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/0b/7d/98268d64c8697425f712c897265394486542141bbe4de319d6\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i2Pou3JHZ4l"
      },
      "source": [
        "import nltk\n",
        "import pycaret"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB2bZLB4umXm"
      },
      "source": [
        "# Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPdVrOSDup0Q"
      },
      "source": [
        "## I. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBGITeID1qQL",
        "outputId": "3f65f0a7-9788-4148-f51f-28b550646a7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# importar el modulo de tokenizacion de nltk\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWoV0d8x2qAF",
        "outputId": "b2bf1d1c-525b-49ea-8ff6-9e87861f9d66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "example_text = \"Esto es un ejemplo. O noooo?\"\n",
        "print(sent_tokenize(example_text))\n",
        "print(word_tokenize(example_text))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Esto es un ejemplo.', 'O noooo?']\n",
            "['Esto', 'es', 'un', 'ejemplo', '.', 'O', 'noooo', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dSxot7HuuWo",
        "outputId": "2c361fe1-a7ec-41d4-a76a-a9843f9c07bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "# Texto de ejemplo. Ene ste caso usamos las tres comillas para separar\n",
        "# el texto en varias lineas dentro de nuestro codigo.\n",
        "example_text = ''' Hola mi nombre es Mario. Mi cuenta de github\n",
        "es uumami. Estas listo para convertirte en un experto de NLP.\n",
        "Ezta horacion contiene muchos erroress gramaticales.\n",
        "'''\n",
        "print(example_text)\n",
        "print(sent_tokenize(example_text))\n",
        "print(word_tokenize(example_text))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Hola mi nombre es Mario. Mi cuenta de github\n",
            "es uumami. Estas listo para convertirte en un experto de NLP.\n",
            "Ezta horacion contiene muchos erroress gramaticales.\n",
            "\n",
            "[' Hola mi nombre es Mario.', 'Mi cuenta de github\\nes uumami.', 'Estas listo para convertirte en un experto de NLP.', 'Ezta horacion contiene muchos erroress gramaticales.']\n",
            "['Hola', 'mi', 'nombre', 'es', 'Mario', '.', 'Mi', 'cuenta', 'de', 'github', 'es', 'uumami', '.', 'Estas', 'listo', 'para', 'convertirte', 'en', 'un', 'experto', 'de', 'NLP', '.', 'Ezta', 'horacion', 'contiene', 'muchos', 'erroress', 'gramaticales', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tj0dRqdGOSf"
      },
      "source": [
        "texto = word_tokenize(example_text)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBTNkxBxIa5o"
      },
      "source": [
        "# II. Spell check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lxqzZ_oRs2_"
      },
      "source": [
        "from autocorrect import Speller"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_Yb6k0GIfi4",
        "outputId": "b0fdd333-84ce-4e51-948e-5e3bfed58307",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "spell = Speller(lang='es')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dictionary for this language not found, downloading...\n",
            "__________________________________________________\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if_n9aZ8Iitl",
        "outputId": "148bdaff-e9d4-4943-92dc-2e31361906c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        }
      },
      "source": [
        "corrected_text = [spell(i) for i in texto]\n",
        "corrected_text"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hola',\n",
              " 'mi',\n",
              " 'nombre',\n",
              " 'es',\n",
              " 'Mario',\n",
              " '.',\n",
              " 'Mi',\n",
              " 'cuenta',\n",
              " 'de',\n",
              " 'github',\n",
              " 'es',\n",
              " 'umami',\n",
              " '.',\n",
              " 'Estas',\n",
              " 'listo',\n",
              " 'para',\n",
              " 'convertirte',\n",
              " 'en',\n",
              " 'un',\n",
              " 'experto',\n",
              " 'de',\n",
              " 'LP',\n",
              " '.',\n",
              " 'Esta',\n",
              " 'horacio',\n",
              " 'contiene',\n",
              " 'muchos',\n",
              " 'errores',\n",
              " 'gramaticales',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVw9RFFEP9l-"
      },
      "source": [
        "## III. Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZAzvIOnRmYG",
        "outputId": "f1b4bb59-6e4d-4465-e817-02b270cafb94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjjDE2POQ2H8",
        "outputId": "c3c3432c-7385-4c49-ea65-5183f8f7f14a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "stopwords_en = stopwords.words('english')\n",
        "\n",
        "counter = 0\n",
        "for w in stopwords_en:\n",
        "  print(w, end=', ')\n",
        "  counter += 1\n",
        "  if (counter%10)== 0:\n",
        "    print()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i, me, my, myself, we, our, ours, ourselves, you, you're, \n",
            "you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, \n",
            "himself, she, she's, her, hers, herself, it, it's, its, itself, \n",
            "they, them, their, theirs, themselves, what, which, who, whom, this, \n",
            "that, that'll, these, those, am, is, are, was, were, be, \n",
            "been, being, have, has, had, having, do, does, did, doing, \n",
            "a, an, the, and, but, if, or, because, as, until, \n",
            "while, of, at, by, for, with, about, against, between, into, \n",
            "through, during, before, after, above, below, to, from, up, down, \n",
            "in, out, on, off, over, under, again, further, then, once, \n",
            "here, there, when, where, why, how, all, any, both, each, \n",
            "few, more, most, other, some, such, no, nor, not, only, \n",
            "own, same, so, than, too, very, s, t, can, will, \n",
            "just, don, don't, should, should've, now, d, ll, m, o, \n",
            "re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, \n",
            "doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, \n",
            "ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, \n",
            "shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't, "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc2il63oQB4q",
        "outputId": "b8e76d48-8b02-49ac-b559-4eba3a26e3de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        }
      },
      "source": [
        "stopwords_es = stopwords.words('spanish')\n",
        "\n",
        "counter = 0\n",
        "for w in stopwords_es:\n",
        "  print(w, end=', ')\n",
        "  counter += 1\n",
        "  if (counter%10)== 0:\n",
        "    print()\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "de, la, que, el, en, y, a, los, del, se, \n",
            "las, por, un, para, con, no, una, su, al, lo, \n",
            "como, más, pero, sus, le, ya, o, este, sí, porque, \n",
            "esta, entre, cuando, muy, sin, sobre, también, me, hasta, hay, \n",
            "donde, quien, desde, todo, nos, durante, todos, uno, les, ni, \n",
            "contra, otros, ese, eso, ante, ellos, e, esto, mí, antes, \n",
            "algunos, qué, unos, yo, otro, otras, otra, él, tanto, esa, \n",
            "estos, mucho, quienes, nada, muchos, cual, poco, ella, estar, estas, \n",
            "algunas, algo, nosotros, mi, mis, tú, te, ti, tu, tus, \n",
            "ellas, nosotras, vosotros, vosotras, os, mío, mía, míos, mías, tuyo, \n",
            "tuya, tuyos, tuyas, suyo, suya, suyos, suyas, nuestro, nuestra, nuestros, \n",
            "nuestras, vuestro, vuestra, vuestros, vuestras, esos, esas, estoy, estás, está, \n",
            "estamos, estáis, están, esté, estés, estemos, estéis, estén, estaré, estarás, \n",
            "estará, estaremos, estaréis, estarán, estaría, estarías, estaríamos, estaríais, estarían, estaba, \n",
            "estabas, estábamos, estabais, estaban, estuve, estuviste, estuvo, estuvimos, estuvisteis, estuvieron, \n",
            "estuviera, estuvieras, estuviéramos, estuvierais, estuvieran, estuviese, estuvieses, estuviésemos, estuvieseis, estuviesen, \n",
            "estando, estado, estada, estados, estadas, estad, he, has, ha, hemos, \n",
            "habéis, han, haya, hayas, hayamos, hayáis, hayan, habré, habrás, habrá, \n",
            "habremos, habréis, habrán, habría, habrías, habríamos, habríais, habrían, había, habías, \n",
            "habíamos, habíais, habían, hube, hubiste, hubo, hubimos, hubisteis, hubieron, hubiera, \n",
            "hubieras, hubiéramos, hubierais, hubieran, hubiese, hubieses, hubiésemos, hubieseis, hubiesen, habiendo, \n",
            "habido, habida, habidos, habidas, soy, eres, es, somos, sois, son, \n",
            "sea, seas, seamos, seáis, sean, seré, serás, será, seremos, seréis, \n",
            "serán, sería, serías, seríamos, seríais, serían, era, eras, éramos, erais, \n",
            "eran, fui, fuiste, fue, fuimos, fuisteis, fueron, fuera, fueras, fuéramos, \n",
            "fuerais, fueran, fuese, fueses, fuésemos, fueseis, fuesen, sintiendo, sentido, sentida, \n",
            "sentidos, sentidas, siente, sentid, tengo, tienes, tiene, tenemos, tenéis, tienen, \n",
            "tenga, tengas, tengamos, tengáis, tengan, tendré, tendrás, tendrá, tendremos, tendréis, \n",
            "tendrán, tendría, tendrías, tendríamos, tendríais, tendrían, tenía, tenías, teníamos, teníais, \n",
            "tenían, tuve, tuviste, tuvo, tuvimos, tuvisteis, tuvieron, tuviera, tuvieras, tuviéramos, \n",
            "tuvierais, tuvieran, tuviese, tuvieses, tuviésemos, tuvieseis, tuviesen, teniendo, tenido, tenida, \n",
            "tenidos, tenidas, tened, "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kw4_Nz3UmJm"
      },
      "source": [
        "def print_text(texx = [], module=10, sep=' '):\n",
        "  counter = 0\n",
        "  for w in texx:\n",
        "    print(w, end=sep)\n",
        "    counter += 1\n",
        "    if (counter % module)== 0:\n",
        "      print()\n",
        "  print('\\n ------- Tamano del texto: ', len(texx))\n",
        "  print()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITCTj4EFULL-",
        "outputId": "6fcca6b9-387c-4790-a718-f0396b2ea368",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "# Removamos las stopwords de nuestro texto\n",
        "print('\\n Texto corregido:')\n",
        "print_text(corrected_text, module=11)\n",
        "\n",
        "short_text = [w for w in corrected_text if w not in stopwords_es]\n",
        "print('\\n Texto sin stopwords:')\n",
        "print_text(short_text, module=11)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Texto corregido:\n",
            "Hola mi nombre es Mario . Mi cuenta de github es \n",
            "umami . Estas listo para convertirte en un experto de LP \n",
            ". Esta horacio contiene muchos errores gramaticales . \n",
            " ------- Tamano del texto:  30\n",
            "\n",
            "\n",
            " Texto sin stopwords:\n",
            "Hola nombre Mario . Mi cuenta github umami . Estas listo \n",
            "convertirte experto LP . Esta horacio contiene errores gramaticales . \n",
            " ------- Tamano del texto:  21\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzxM8D-HEvA-"
      },
      "source": [
        "## IV. Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqzc2xAj8uyt"
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer('spanish')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNd5c-nyMJ1W",
        "outputId": "bc5619a4-0ce4-4c66-b1aa-d505a322a0ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "[stemmer.stem(i) for i in ['corriendo', 'correr', 'corremos',\n",
        "                           'corriamos','corrian', \n",
        "                           'correlacion', 'correlacionado']]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['corr', 'corr', 'corr', 'corri', 'corri', 'correlacion', 'correlacion']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0srEErAF0Nv",
        "outputId": "58a0a375-e8ae-4461-ab31-dde3e7a5f816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "stemmed_text = [stemmer.stem(i) for i in corrected_text]\n",
        "print_text(stemmed_text, module=11)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hol mi nombr es mari . mi cuent de github es \n",
            "umami . estas list par convertirt en un expert de lp \n",
            ". esta horaci contien much error gramatical . \n",
            " ------- Tamano del texto:  30\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVDE_3X7p5wQ"
      },
      "source": [
        "## V. Lemmatizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0fWcC8hrXkl"
      },
      "source": [
        "Dado que el texto que estamos anlizaando se encuenntra en espanol, es necesario buscar una funcion disenyada para nuestra lengua.  \n",
        "En este nos puede ayudar spacy.  \n",
        "+ spacy: https://spacy.io/usage \n",
        "+ spanish lemmatizer: https://spacy.io/models/es#es_core_news_sm "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYa1_Y_HuoXa"
      },
      "source": [
        "!python3 -m spacy download es_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_K-usVxvVhY"
      },
      "source": [
        "# Importar spacy y el lemmatizer en espanyol\n",
        "import spacy\n",
        "import es_core_news_sm\n",
        "nlp = es_core_news_sm.load()"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiKoAF0p1RyP",
        "outputId": "e973980b-c753-435d-d801-8816ab90390f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "ex_lem = \"Hola estoy aprendiendo a utilizar inteligencia artificial. \\n\"\n",
        "print(ex_lem)\n",
        "ex_lem = word_tokenize(ex_lem)\n",
        "for w in ex_lem:\n",
        "  doc = nlp(w)\n",
        "  for word in doc:\n",
        "    print(word.text, \"=>\", word.lemma_)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hola estoy aprendiendo a utilizar inteligencia artificial. \n",
            "\n",
            "Hola => Hola\n",
            "estoy => estar\n",
            "aprendiendo => aprender\n",
            "a => a\n",
            "utilizar => utilizar\n",
            "inteligencia => inteligencia\n",
            "artificial => artificial\n",
            ". => .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzoQIpJc_o_W"
      },
      "source": [
        "Creemos una funcion que lemamtize un texto que ya este separado en tokens (tokenized/tokenizado)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKdHUg2D3O7h"
      },
      "source": [
        "def lemm_es(txx=[]):\n",
        "  '''\n",
        "  txx is a list or tokenized text\n",
        "  retunrs: lemmatized words.\n",
        "  '''\n",
        "  lemm_text = []\n",
        "  for w in txx:\n",
        "    lemm_text.append(nlp(w)[0].lemma_)\n",
        "  return lemm_text"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RsBRAdN31Mg",
        "outputId": "5fcc2cf0-d181-40e3-924a-077c0fea719d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "lemm_text = lemm_es(short_text)\n",
        "\n",
        "print_text(short_text, module=11)\n",
        "print_text(lemm_text, module=11)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hola nombre Mario . Mi cuenta github umami . Estas listo \n",
            "convertirte experto LP . Esta horacio contiene errores gramaticales . \n",
            " ------- Tamano del texto:  21\n",
            "\n",
            "Hola nombrar Mario . Mi contar github umami . Estas listar \n",
            "convertirte experto LP . Esta horacio contener error gramatical . \n",
            " ------- Tamano del texto:  21\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5R3T4XC1KT4"
      },
      "source": [
        "Apliquemos el lemmatizer a nuestro texto corregido, y sin stopwords."
      ]
    }
  ]
}