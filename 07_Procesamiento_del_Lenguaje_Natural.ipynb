{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_Procesamiento_del_Lenguaje_Natural.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM6cXbtNNpE9DDhuJ0PmK10",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IEXE-Tec/aprendizaje-maquina-2/blob/master/07_Procesamiento_del_Lenguaje_Natural.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kECscepGtsRT"
      },
      "source": [
        "Installing pycaret library:\n",
        "+ https://towardsdatascience.com/announcing-pycaret-2-0-39c11014540e \n",
        "+ https://pycaret.org/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_4YZ-A-ty8L"
      },
      "source": [
        "!pip install pycaret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUw9jGQ6e8zi",
        "outputId": "d7685e23-942c-4553-9c24-437803260179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "from pycaret.utils import version\n",
        "version()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.1.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA99ILJFHqa3"
      },
      "source": [
        "Installing Autocorrect:\n",
        "+ https://github.com/phatpiglet/autocorrect\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX_UZpHaHtYN",
        "outputId": "3c7bdae7-699f-47a5-e224-1371047e8cff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "!pip install autocorrect"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspellchecker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/96/827c132397d0eb5731c1eda05dbfb019ede064ca8c7d0f329160ce0a4acd/pyspellchecker-0.5.5-py2.py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.5.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i2Pou3JHZ4l"
      },
      "source": [
        "import nltk\n",
        "import pycaret\n",
        "from spellchecker import SpellChecker"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB2bZLB4umXm"
      },
      "source": [
        "# Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPdVrOSDup0Q"
      },
      "source": [
        "## I. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBGITeID1qQL"
      },
      "source": [
        "# importar el modulo de tokenizacion de nltk\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWoV0d8x2qAF",
        "outputId": "7b61f488-5e0e-430a-a921-5ee1e77d785f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "example_text = \"Esto es un ejemplo. O noooo?\"\n",
        "print(sent_tokenize(example_text))\n",
        "print(word_tokenize(example_text))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Esto es un ejemplo.', 'O noooo?']\n",
            "['Esto', 'es', 'un', 'ejemplo', '.', 'O', 'noooo', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dSxot7HuuWo",
        "outputId": "601e795f-f906-478a-8e73-4b0abbd2cf91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "# Texto de ejemplo. Ene ste caso usamos las tres comillas para separar\n",
        "# el texto en varias lineas dentro de nuestro codigo.\n",
        "example_text = ''' Hola mi nombre es Mario. Mi cuenta de github\n",
        "es uumami. Estas listo para convertirte en un experto de NLP.\n",
        "Ezta horacion contiene muchos erroress gramaticales.\n",
        "'''\n",
        "print(example_text)\n",
        "print(sent_tokenize(example_text))\n",
        "print(word_tokenize(example_text))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Hola mi nombre es Mario. Mi cuenta de github\n",
            "es uumami. Estas listo para convertirte en un experto de NLP.\n",
            "Ezta horacion contiene muchos erroress gramaticales.\n",
            "\n",
            "[' Hola mi nombre es Mario.', 'Mi cuenta de github\\nes uumami.', 'Estas listo para convertirte en un experto de NLP.', 'Ezta horacion contiene muchos erroress gramaticales.']\n",
            "['Hola', 'mi', 'nombre', 'es', 'Mario', '.', 'Mi', 'cuenta', 'de', 'github', 'es', 'uumami', '.', 'Estas', 'listo', 'para', 'convertirte', 'en', 'un', 'experto', 'de', 'NLP', '.', 'Ezta', 'horacion', 'contiene', 'muchos', 'erroress', 'gramaticales', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tj0dRqdGOSf"
      },
      "source": [
        "texto = word_tokenize(example_text)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBTNkxBxIa5o"
      },
      "source": [
        "# II. Spell check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_Yb6k0GIfi4"
      },
      "source": [
        "from autocorrect import Speller\n",
        "spell = Speller(lang='es')"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if_n9aZ8Iitl",
        "outputId": "57f35a5d-26af-41a7-a836-bbd4e22e9163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        }
      },
      "source": [
        "corrected_text = [spell(i) for i in texto]\n",
        "corrected_text"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hola',\n",
              " 'mi',\n",
              " 'nombre',\n",
              " 'es',\n",
              " 'Mario',\n",
              " '.',\n",
              " 'Mi',\n",
              " 'cuenta',\n",
              " 'de',\n",
              " 'github',\n",
              " 'es',\n",
              " 'umami',\n",
              " '.',\n",
              " 'Estas',\n",
              " 'listo',\n",
              " 'para',\n",
              " 'convertirte',\n",
              " 'en',\n",
              " 'un',\n",
              " 'experto',\n",
              " 'de',\n",
              " 'LP',\n",
              " '.',\n",
              " 'Esta',\n",
              " 'horacio',\n",
              " 'contiene',\n",
              " 'muchos',\n",
              " 'errores',\n",
              " 'gramaticales',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzxM8D-HEvA-"
      },
      "source": [
        "## III. Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqzc2xAj8uyt"
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer('spanish')"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNd5c-nyMJ1W",
        "outputId": "7afb9ecc-bcac-4bd5-cca6-6e6aabd2eb7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "[stemmer.stem(i) for i in ['corriendo', 'correr', 'corremos',\n",
        "                           'corriamos','corrian', \n",
        "                           'correlacion', 'correlacionado']]"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['corr', 'corr', 'corr', 'corri', 'corri', 'correlacion', 'correlacion']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0srEErAF0Nv",
        "outputId": "24075baa-7fab-4eab-d020-35abd2134816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        }
      },
      "source": [
        "stemmed_text = [stemmer.stem(i) for i in corrected_text]\n",
        "stemmed_text"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hol',\n",
              " 'mi',\n",
              " 'nombr',\n",
              " 'es',\n",
              " 'mari',\n",
              " '.',\n",
              " 'mi',\n",
              " 'cuent',\n",
              " 'de',\n",
              " 'github',\n",
              " 'es',\n",
              " 'umami',\n",
              " '.',\n",
              " 'estas',\n",
              " 'list',\n",
              " 'par',\n",
              " 'convertirt',\n",
              " 'en',\n",
              " 'un',\n",
              " 'expert',\n",
              " 'de',\n",
              " 'lp',\n",
              " '.',\n",
              " 'esta',\n",
              " 'horaci',\n",
              " 'contien',\n",
              " 'much',\n",
              " 'error',\n",
              " 'gramatical',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    }
  ]
}